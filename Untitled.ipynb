{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fb50980-b6ee-4515-8367-f7abd78c6677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set your target URL (e.g., Azerbaijani plov)\n",
    "url = \"https://www.pexels.com/search/dolma/\"\n",
    "\n",
    "# Create folder for images\n",
    "folder_name = \"dolma_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Send request\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find image tags\n",
    "images = soup.find_all(\"img\")\n",
    "\n",
    "# Download images\n",
    "for i, img in enumerate(images):\n",
    "    img_url = \"https:\" + img.get(\"src\")\n",
    "    \n",
    "    # Ignore tiny thumbnails\n",
    "    if \"thumb\" in img_url and not img_url.endswith(\".svg\"):\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(f\"{folder_name}/dolma_{i}.jpg\", \"wb\") as handler:\n",
    "            handler.write(img_data)\n",
    "        print(f\"Downloaded image {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b23a9b3-ca02-44c8-9386-410e514e3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: dolma_0.jpg\n",
      "Downloaded: dolma_1.jpg\n",
      "Downloaded: dolma_2.jpg\n",
      "Downloaded: dolma_3.jpg\n",
      "Downloaded: dolma_4.jpg\n",
      "Downloaded: dolma_5.jpg\n",
      "Downloaded: dolma_6.jpg\n",
      "Downloaded: dolma_7.jpg\n",
      "Downloaded: dolma_8.jpg\n",
      "Downloaded: dolma_9.jpg\n",
      "Downloaded: dolma_10.jpg\n",
      "Downloaded: dolma_11.jpg\n",
      "Downloaded: dolma_12.jpg\n",
      "Downloaded: dolma_13.jpg\n",
      "Downloaded: dolma_14.jpg\n",
      "Downloaded: dolma_15.jpg\n",
      "Downloaded: dolma_16.jpg\n",
      "Downloaded: dolma_17.jpg\n",
      "Downloaded: dolma_18.jpg\n",
      "Downloaded: dolma_19.jpg\n",
      "Downloaded: dolma_20.jpg\n",
      "Downloaded: dolma_21.jpg\n",
      "Downloaded: dolma_22.jpg\n",
      "Downloaded: dolma_23.jpg\n",
      "Downloaded: dolma_24.jpg\n",
      "Downloaded: dolma_25.jpg\n",
      "Downloaded: dolma_26.jpg\n",
      "Downloaded: dolma_27.jpg\n",
      "Downloaded: dolma_28.jpg\n",
      "Downloaded: dolma_29.jpg\n",
      "Downloaded: dolma_30.jpg\n",
      "Downloaded: dolma_31.jpg\n",
      "Downloaded: dolma_32.jpg\n",
      "Downloaded: dolma_33.jpg\n",
      "Downloaded: dolma_34.jpg\n",
      "Downloaded: dolma_35.jpg\n",
      "Downloaded: dolma_36.jpg\n",
      "Downloaded: dolma_37.jpg\n",
      "Downloaded: dolma_38.jpg\n",
      "Downloaded: dolma_39.jpg\n",
      "Downloaded: dolma_40.jpg\n",
      "Downloaded: dolma_41.jpg\n",
      "Downloaded: dolma_42.jpg\n",
      "Downloaded: dolma_43.jpg\n",
      "Downloaded: dolma_44.jpg\n",
      "Downloaded: dolma_45.jpg\n",
      "Downloaded: dolma_46.jpg\n",
      "Downloaded: dolma_47.jpg\n",
      "Downloaded: dolma_48.jpg\n",
      "Downloaded: dolma_49.jpg\n",
      "Downloaded: dolma_50.jpg\n",
      "Downloaded: dolma_51.jpg\n",
      "Downloaded: dolma_52.jpg\n",
      "Downloaded: dolma_53.jpg\n",
      "Downloaded: dolma_54.jpg\n",
      "Downloaded: dolma_55.jpg\n",
      "Downloaded: dolma_56.jpg\n",
      "Downloaded: dolma_57.jpg\n",
      "Downloaded: dolma_58.jpg\n",
      "Downloaded: dolma_59.jpg\n",
      "Downloaded: dolma_60.jpg\n",
      "Downloaded: dolma_61.jpg\n",
      "Downloaded: dolma_62.jpg\n",
      "Downloaded: dolma_63.jpg\n",
      "Downloaded: dolma_64.jpg\n",
      "Downloaded: dolma_65.jpg\n",
      "Downloaded: dolma_66.jpg\n",
      "Downloaded: dolma_67.jpg\n",
      "Downloaded: dolma_68.jpg\n",
      "Downloaded: dolma_69.jpg\n",
      "Downloaded: dolma_70.jpg\n",
      "Downloaded: dolma_71.jpg\n",
      "Downloaded: dolma_72.jpg\n",
      "Downloaded: dolma_73.jpg\n",
      "Downloaded: dolma_74.jpg\n",
      "Downloaded: dolma_75.jpg\n",
      "Downloaded: dolma_76.jpg\n",
      "Downloaded: dolma_77.jpg\n",
      "Downloaded: dolma_78.jpg\n",
      "Downloaded: dolma_79.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "API_KEY = 'QkrxdOUS02cj82AFbH6JwYLR8Bhl3nKsVF3gyH4oKhOiQzaVQ7pzdXvK'\n",
    "headers = {\n",
    "    'Authorization': API_KEY\n",
    "}\n",
    "query = 'dolma'\n",
    "per_page = 100\n",
    "url = f'https://api.pexels.com/v1/search?query={query}&per_page={per_page}'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "folder_name = \"dolma_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for i, photo in enumerate(data['photos']):\n",
    "    img_url = photo['src']['original']\n",
    "    img_data = requests.get(img_url).content\n",
    "    with open(f\"{folder_name}/dolma_{i}.jpg\", \"wb\") as handler:\n",
    "        handler.write(img_data)\n",
    "    print(f\"Downloaded: dolma_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "273e978c-f873-4e57-aec7-8c1b522c202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set your target URL (e.g., Azerbaijani plov)\n",
    "url = \"https://www.pexels.com/search/plov/\"\n",
    "\n",
    "# Create folder for images\n",
    "folder_name = \"plov_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Send request\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find image tags\n",
    "images = soup.find_all(\"img\")\n",
    "\n",
    "# Download images\n",
    "for i, img in enumerate(images):\n",
    "    img_url = \"https:\" + img.get(\"src\")\n",
    "    \n",
    "    # Ignore tiny thumbnails\n",
    "    if \"thumb\" in img_url and not img_url.endswith(\".svg\"):\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(f\"{folder_name}/dolma_{i}.jpg\", \"wb\") as handler:\n",
    "            handler.write(img_data)\n",
    "        print(f\"Downloaded image {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c1ffcf6-57f8-4b40-b2ef-b5a8dc2ab71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: plov_0.jpg\n",
      "Downloaded: plov_1.jpg\n",
      "Downloaded: plov_2.jpg\n",
      "Downloaded: plov_3.jpg\n",
      "Downloaded: plov_4.jpg\n",
      "Downloaded: plov_5.jpg\n",
      "Downloaded: plov_6.jpg\n",
      "Downloaded: plov_7.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "API_KEY = 'QkrxdOUS02cj82AFbH6JwYLR8Bhl3nKsVF3gyH4oKhOiQzaVQ7pzdXvK'\n",
    "headers = {\n",
    "    'Authorization': API_KEY\n",
    "}\n",
    "query = 'plov'\n",
    "per_page = 8\n",
    "url = f'https://api.pexels.com/v1/search?query={query}&per_page={per_page}'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "folder_name = \"plov_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "for i, photo in enumerate(data['photos']):\n",
    "    img_url = photo['src']['original']\n",
    "    img_data = requests.get(img_url).content\n",
    "    with open(f\"{folder_name}/plov_{i}.jpg\", \"wb\") as handler:\n",
    "        handler.write(img_data)\n",
    "    print(f\"Downloaded: plov_{i}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db092663-d87c-4493-ac34-5d905203ec86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[API] Downloaded: kabab_api_0.jpg\n",
      "[API] Downloaded: kabab_api_1.jpg\n",
      "[API] Downloaded: kabab_api_2.jpg\n",
      "[API] Downloaded: kabab_api_3.jpg\n",
      "[API] Downloaded: kabab_api_4.jpg\n",
      "[API] Downloaded: kabab_api_5.jpg\n",
      "[API] Downloaded: kabab_api_6.jpg\n",
      "[API] Downloaded: kabab_api_7.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------------- PART 1: Basic Scraping (may have limited results) -------------\n",
    "url = \"https://www.pexels.com/search/kebab/\"\n",
    "folder_name = \"kabab_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "images = soup.find_all(\"img\")\n",
    "\n",
    "for i, img in enumerate(images):\n",
    "    img_url = img.get(\"src\")\n",
    "    if img_url and \"thumb\" in img_url and not img_url.endswith(\".svg\"):\n",
    "        img_url = \"https:\" + img_url if img_url.startswith(\"//\") else img_url\n",
    "        img_data = requests.get(img_url).content\n",
    "        with open(f\"{folder_name}/kabab_scraped_{i}.jpg\", \"wb\") as handler:\n",
    "            handler.write(img_data)\n",
    "        print(f\"[Scraped] Downloaded: kabab_scraped_{i}.jpg\")\n",
    "\n",
    "# ------------- PART 2: Using Pexels API (Recommended Way) -------------\n",
    "API_KEY = 'QkrxdOUS02cj82AFbH6JwYLR8Bhl3nKsVF3gyH4oKhOiQzaVQ7pzdXvK'  # Replace with your key if needed\n",
    "headers = {\n",
    "    'Authorization': API_KEY\n",
    "}\n",
    "query = 'kabab'\n",
    "per_page = 50\n",
    "url = f'https://api.pexels.com/v1/search?query={query}&per_page={per_page}'\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "data = response.json()\n",
    "\n",
    "for i, photo in enumerate(data['photos']):\n",
    "    img_url = photo['src']['original']\n",
    "    img_data = requests.get(img_url).content\n",
    "    with open(f\"{folder_name}/kabab_api_{i}.jpg\", \"wb\") as handler:\n",
    "        handler.write(img_data)\n",
    "    print(f\"[API] Downloaded: kabab_api_{i}.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d982973-5c7d-4848-8374-e539e959e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in d:\\jupyt\\lib\\site-packages (4.31.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in d:\\jupyt\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.3)\n",
      "Requirement already satisfied: trio~=0.17 in d:\\jupyt\\lib\\site-packages (from selenium) (0.29.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in d:\\jupyt\\lib\\site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in d:\\jupyt\\lib\\site-packages (from selenium) (2024.8.30)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in d:\\jupyt\\lib\\site-packages (from selenium) (4.11.0)\n",
      "Requirement already satisfied: websocket-client~=1.8 in d:\\jupyt\\lib\\site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: attrs>=23.2.0 in d:\\jupyt\\lib\\site-packages (from trio~=0.17->selenium) (25.3.0)\n",
      "Requirement already satisfied: sortedcontainers in d:\\jupyt\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in d:\\jupyt\\lib\\site-packages (from trio~=0.17->selenium) (3.7)\n",
      "Requirement already satisfied: outcome in d:\\jupyt\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in d:\\jupyt\\lib\\site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: cffi>=1.14 in d:\\jupyt\\lib\\site-packages (from trio~=0.17->selenium) (1.17.1)\n",
      "Requirement already satisfied: wsproto>=0.14 in d:\\jupyt\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in d:\\jupyt\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: pycparser in d:\\jupyt\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in d:\\jupyt\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Requirement already satisfied: webdriver-manager in d:\\jupyt\\lib\\site-packages (4.0.2)\n",
      "Requirement already satisfied: requests in d:\\jupyt\\lib\\site-packages (from webdriver-manager) (2.32.3)\n",
      "Requirement already satisfied: python-dotenv in d:\\jupyt\\lib\\site-packages (from webdriver-manager) (0.21.0)\n",
      "Requirement already satisfied: packaging in d:\\jupyt\\lib\\site-packages (from webdriver-manager) (24.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\jupyt\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\jupyt\\lib\\site-packages (from requests->webdriver-manager) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\jupyt\\lib\\site-packages (from requests->webdriver-manager) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\jupyt\\lib\\site-packages (from requests->webdriver-manager) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium\n",
    "!pip install webdriver-manager\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80e6c80-74da-46cb-b8a9-84eee1d62628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading image 0: Invalid URL 'https:///static/images/icons/wikipedia.png': No host supplied\n",
      "Downloaded: az_cuisine_3.jpg\n",
      "Downloaded: az_cuisine_4.jpg\n",
      "Downloaded: az_cuisine_5.jpg\n",
      "Downloaded: az_cuisine_6.jpg\n",
      "Downloaded: az_cuisine_7.jpg\n",
      "Downloaded: az_cuisine_8.jpg\n",
      "Downloaded: az_cuisine_9.jpg\n",
      "Downloaded: az_cuisine_10.jpg\n",
      "Downloaded: az_cuisine_11.jpg\n",
      "Downloaded: az_cuisine_12.jpg\n",
      "Downloaded: az_cuisine_13.jpg\n",
      "Downloaded: az_cuisine_14.jpg\n",
      "Downloaded: az_cuisine_15.jpg\n",
      "Downloaded: az_cuisine_16.jpg\n",
      "Downloaded: az_cuisine_17.jpg\n",
      "Downloaded: az_cuisine_18.jpg\n",
      "Downloaded: az_cuisine_19.jpg\n",
      "Downloaded: az_cuisine_20.jpg\n",
      "Downloaded: az_cuisine_21.jpg\n",
      "Downloaded: az_cuisine_22.jpg\n",
      "Downloaded: az_cuisine_23.jpg\n",
      "Downloaded: az_cuisine_24.jpg\n",
      "Downloaded: az_cuisine_25.jpg\n",
      "Downloaded: az_cuisine_26.jpg\n",
      "Downloaded: az_cuisine_27.jpg\n",
      "Downloaded: az_cuisine_28.jpg\n",
      "Downloaded: az_cuisine_29.jpg\n",
      "Downloaded: az_cuisine_30.jpg\n",
      "Downloaded: az_cuisine_31.jpg\n",
      "Downloaded: az_cuisine_32.jpg\n",
      "Downloaded: az_cuisine_33.jpg\n",
      "Downloaded: az_cuisine_34.jpg\n",
      "Downloaded: az_cuisine_35.jpg\n",
      "Downloaded: az_cuisine_36.jpg\n",
      "Downloaded: az_cuisine_37.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Set up headless Chrome browser\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Comment out to see browser window\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "# Target URL\n",
    "url = \"https://en.wikipedia.org/wiki/Azerbaijani_cuisine\"\n",
    "driver.get(url)\n",
    "time.sleep(2)  # Let the page load\n",
    "\n",
    "# Scroll to bottom to load lazy images\n",
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "time.sleep(2)\n",
    "\n",
    "# Get HTML after JS execution\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Create folder\n",
    "folder_name = \"azerbaijani_cuisine_selenium\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Find and download images\n",
    "images = soup.find_all(\"img\")\n",
    "for i, img in enumerate(images):\n",
    "    src = img.get(\"src\")\n",
    "    if src and not src.endswith(\".svg\"):\n",
    "        img_url = urljoin(\"https:\", src)\n",
    "        try:\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f\"{folder_name}/az_cuisine_{i}.jpg\", \"wb\") as f:\n",
    "                f.write(img_data)\n",
    "            print(f\"Downloaded: az_cuisine_{i}.jpg\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading image {i}: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6691cb9-018c-4929-b5ff-45fa1d136be9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fa26e05-b1f9-4c70-92c6-2caef45afed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded: kebab_0.jpg\n",
      "Downloaded: kebab_1.jpg\n",
      "Downloaded: kebab_2.jpg\n",
      "Downloaded: kebab_3.jpg\n",
      "Downloaded: kebab_4.jpg\n",
      "Downloaded: kebab_5.jpg\n",
      "Downloaded: kebab_6.jpg\n",
      "Downloaded: kebab_7.jpg\n",
      "Downloaded: kebab_8.jpg\n",
      "Downloaded: kebab_9.jpg\n",
      "Downloaded: kebab_10.jpg\n",
      "Downloaded: kebab_11.jpg\n",
      "Downloaded: kebab_12.jpg\n",
      "Downloaded: kebab_13.jpg\n",
      "Downloaded: kebab_14.jpg\n",
      "Downloaded: kebab_15.jpg\n",
      "Downloaded: kebab_16.jpg\n",
      "Downloaded: kebab_17.jpg\n",
      "Downloaded: kebab_18.jpg\n",
      "Downloaded: kebab_19.jpg\n",
      "Downloaded: kebab_20.jpg\n",
      "Downloaded: kebab_21.jpg\n",
      "Downloaded: kebab_22.jpg\n",
      "Downloaded: kebab_23.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- Setup ---\n",
    "search_query = \"kebab\"\n",
    "url = f\"https://www.pexels.com/search/{search_query}/\"\n",
    "folder_name = f\"{search_query}_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# --- Selenium Options ---\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Optional: \"--headless\" for headless mode\n",
    "\n",
    "# --- Start Driver ---\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# --- Scroll to load more images ---\n",
    "for _ in range(10):  # Increase this number if needed\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# --- Get all image elements ---\n",
    "img_elements = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "\n",
    "# --- Download valid images ---\n",
    "downloaded = 0\n",
    "for i, img in enumerate(img_elements):\n",
    "    img_url = img.get_attribute(\"src\") or img.get_attribute(\"data-src\") or img.get_attribute(\"srcset\")\n",
    "    \n",
    "    if img_url and \"images.pexels.com\" in img_url:\n",
    "        try:\n",
    "            if \"?\" in img_url:\n",
    "                img_url = img_url.split(\"?\")[0]  # Clean URL\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f\"{folder_name}/{search_query}_{downloaded}.jpg\", \"wb\") as handler:\n",
    "                handler.write(img_data)\n",
    "            print(f\"Downloaded: {search_query}_{downloaded}.jpg\")\n",
    "            downloaded += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_url}: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "507f332c-daf3-4c91-985f-7f33f9b15511",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- Setup ---\n",
    "search_query = \"paxlava\"\n",
    "url = f\"https://www.pexels.com/search/{search_query}/\"\n",
    "folder_name = f\"{search_query}_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# --- Selenium Options ---\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Optional: \"--headless\" for headless mode\n",
    "\n",
    "# --- Start Driver ---\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# --- Scroll to load more images ---\n",
    "for _ in range(10):  # Increase this number if needed\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# --- Get all image elements ---\n",
    "img_elements = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "\n",
    "# --- Download valid images ---\n",
    "downloaded = 0\n",
    "for i, img in enumerate(img_elements):\n",
    "    img_url = img.get_attribute(\"src\") or img.get_attribute(\"data-src\") or img.get_attribute(\"srcset\")\n",
    "    \n",
    "    if img_url and \"images.pexels.com\" in img_url:\n",
    "        try:\n",
    "            if \"?\" in img_url:\n",
    "                img_url = img_url.split(\"?\")[0]  # Clean URL\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f\"{folder_name}/{search_query}_{downloaded}.jpg\", \"wb\") as handler:\n",
    "                handler.write(img_data)\n",
    "            print(f\"Downloaded: {search_query}_{downloaded}.jpg\")\n",
    "            downloaded += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_url}: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ca03931-efff-42b2-8058-39a72199f135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- Setup ---\n",
    "search_query = \"sekerbura\"\n",
    "url = f\"https://www.pexels.com/search/{search_query}/\"\n",
    "folder_name = f\"{search_query}_images\"\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# --- Selenium Options ---\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")  # Optional: \"--headless\" for headless mode\n",
    "\n",
    "# --- Start Driver ---\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "driver.get(url)\n",
    "time.sleep(3)\n",
    "\n",
    "# --- Scroll to load more images ---\n",
    "for _ in range(10):  # Increase this number if needed\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# --- Get all image elements ---\n",
    "img_elements = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "\n",
    "# --- Download valid images ---\n",
    "downloaded = 0\n",
    "for i, img in enumerate(img_elements):\n",
    "    img_url = img.get_attribute(\"src\") or img.get_attribute(\"data-src\") or img.get_attribute(\"srcset\")\n",
    "    \n",
    "    if img_url and \"images.pexels.com\" in img_url:\n",
    "        try:\n",
    "            if \"?\" in img_url:\n",
    "                img_url = img_url.split(\"?\")[0]  # Clean URL\n",
    "            img_data = requests.get(img_url).content\n",
    "            with open(f\"{folder_name}/{search_query}_{downloaded}.jpg\", \"wb\") as handler:\n",
    "                handler.write(img_data)\n",
    "            print(f\"Downloaded: {search_query}_{downloaded}.jpg\")\n",
    "            downloaded += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {img_url}: {e}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a082ceb0-5f61-4ed7-900f-14b7e9a90a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
